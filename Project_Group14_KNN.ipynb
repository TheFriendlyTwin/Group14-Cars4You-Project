{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6528948",
   "metadata": {},
   "source": [
    "# Cars 4 You"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821038fc",
   "metadata": {},
   "source": [
    "<a id=\"contribution\">    </a>\n",
    "## Group 14 Member Contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbfde09",
   "metadata": {},
   "source": [
    "What part(s) of the work were done by each member and an estimated % contribution of each member towards the final work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b3a26a",
   "metadata": {},
   "source": [
    "<a id=\"abstract\">    </a>\n",
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5040970",
   "metadata": {},
   "source": [
    "A small summary of your work (200 to 300 words). The abstract should give an overview of your work: What is the context? What are your goals? What did you do? What were your main results, and what conclusions did you draw from them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfd18c1",
   "metadata": {},
   "source": [
    "<a id=\"libraries\">    </a>\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c4fbc-6609-402f-bf0f-4a901ec089ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "sns.set()\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# data partition\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# filter methods: spearman and chi-square\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# ANOVA test\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# wrapper methods\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# embedded methods\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set random seed for reproducibility\n",
    "RSEED = 42\n",
    "np.random.seed(RSEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108b9f49",
   "metadata": {},
   "source": [
    "<a id=\"data\">    </a>\n",
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a452a0-6f3e-464f-b246-2c3587723be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv', sep = \",\")\n",
    "test_data = pd.read_csv('data/test.csv', sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c74269-a51d-450f-940e-cf5193bab046",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bdf063",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bad7263",
   "metadata": {},
   "source": [
    "<a id=\"metada\">    </a>\n",
    "## Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bfd34f-224c-4e4f-9aa9-f4c1e083f42b",
   "metadata": {},
   "source": [
    "`carID` : An attribute that contains an identifier for each car. <br>\n",
    "`Brand` : The car’s main brand (e.g. Ford, Toyota). <br>\n",
    "`model` : The car model. <br>\n",
    "`year`: The year of Registration of the Car. <br>\n",
    "`price (Output)` : The car’s price when purchased by Cars 4 You (in £). <br>\n",
    "`transmission` : The kind of transmission (Manual, Semi-auto, Auto). <br>\n",
    "`mileage`: The total reported distance travelled by the car (in miles). <br>\n",
    "`fuelType`: Type of Fuel used by the car (Diesel, Petrol, Hybrid, Electric). <br>\n",
    "`tax`: The amount of road tax (in £) that, in 2020, was applicable to the car in question. <br>\n",
    "`mpg`: Average Miles per Gallon. <br>\n",
    "`engineSize`: Size of Engine in liters (Cubic Decimeters). <br>\n",
    "`paintQuality%`: The mechanic’s assessment of the cars’ overall paint quality and hull integrity (filled by the mechanic during evaluation). <br>\n",
    "`previousOwners`: Number of previous registered owners of the vehicle. <br>\n",
    "`hasDamage`: Boolean marker filled by the seller at the time of registration stating whether the car is damaged or not. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cbd6b4",
   "metadata": {},
   "source": [
    "### Drop features assessed by the mechanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2720b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop('paintQuality%', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58690f",
   "metadata": {},
   "source": [
    "<a id=\"columns\">    </a>\n",
    "### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d31e3e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function that renames the df columns\n",
    "def rename_columns(df):\n",
    "    \"\"\" This function receives a DataFrame with the cars data as input and renames its columns.\"\"\"\n",
    "\n",
    "    # Rename columns for easier access\n",
    "    df.rename(columns = {'carID': 'car_id',\n",
    "                           'Brand': 'brand',\n",
    "                           'fuelType': 'fuel_type',\n",
    "                           'engineSize': 'engine_size',\n",
    "                           'previousOwners': 'previous_owners',\n",
    "                           'hasDamage': 'has_damage'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b8850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to dataset and confirm\n",
    "rename_columns(train_data)\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7243a7c4",
   "metadata": {},
   "source": [
    "<a id=\"index\">    </a>\n",
    "### Change index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4b0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if ID's are unique\n",
    "len(train_data) == len(train_data.car_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d636cc50-1955-485e-afd8-2ca3a09069f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variable carID as the new index\n",
    "def change_index(df):\n",
    "    ''' This function receives a DataFrame with the cars data as input and sets the column car_id as the new index. '''\n",
    "    \n",
    "    df.set_index('car_id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to dataset and confirm\n",
    "change_index(train_data)\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf8ca8e",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "- [1. Identifying Business Needs](#1)\n",
    "- [2. Data Exploration and Preprocessing](#2)\n",
    "    - [2.1. Explore the data](#2.1)\n",
    "        - [2.1.1. Inspecting data shape, data types and missing values](#2.1.1)\n",
    "        - [2.1.2. Basic descriptive statistics](#2.1.2)\n",
    "        - [2.1.3. Identifying duplicated records](#2.1.3)\n",
    "        - [2.1.4. Check the correlations](#2.1.4)\n",
    "            - [2.1.4.1. Correlation between the target and the numerical variables](#2.1.4.1)\n",
    "            - [2.1.4.2. Correlation between the target and the categorical variables](#2.1.4.2)\n",
    "    - [2.2. Modify the data](#2.2)\n",
    "        - [2.2.1. Correct categorical values](#2.2.1)\n",
    "            - [2.2.1.1. fuel_type](#2.2.1.1)\n",
    "            - [2.2.1.2. transmission](#2.2.1.2)\n",
    "            - [2.2.1.3. brand](#2.2.1.3)\n",
    "            - [2.2.1.4. model](#2.2.1.4)\n",
    "        - [2.2.2. Change data types](#2.2.2)\n",
    "        - [2.2.3. Fix mistakes](#2.2.3)\n",
    "            - [2.2.3.1. previous_owners, mileage, mpg, engine_size, tax: have negative values](#2.2.3.1)\n",
    "            - [2.2.3.2. paint_quality: has percentage bigger than 100](#2.2.3.2)\n",
    "            - [2.2.3.3. has_damage: is always 0](#2.2.3.3)\n",
    "        - [2.2.4. Remove outliers](#2.2.4)\n",
    "            - [2.2.4.1. Winsorizing the outliers in numeric columns](#2.2.4.1)\n",
    "        - [2.2.5. Data separation](#2.2.5)\n",
    "        - [2.2.6. Fill missing values](#2.2.6)\n",
    "            - [2.2.6.1. Year](#2.2.6.1)\n",
    "            - [2.2.6.2. Mileage](#2.2.6.2)\n",
    "            - [2.2.6.3. Tax](#2.2.6.3)\n",
    "            - [2.2.6.4. Mpg](#2.2.6.4)\n",
    "            - [2.2.6.5. Engine size](#2.2.6.5)\n",
    "            - [2.2.6.6. Paint quality](#2.2.6.6)\n",
    "            - [2.2.6.7. Previous owners](#2.2.6.7)\n",
    "            - [2.2.6.8. Has damage](#2.2.6.8)\n",
    "            - [2.2.6.9. Brand](#2.2.6.9)\n",
    "            - [2.2.6.10. Model](#2.2.6.10)\n",
    "            - [2.2.6.11.Transmission](#2.2.6.11)\n",
    "            - [2.2.6.12. Fuel type](#2.2.6.12)\n",
    "            - [2.2.6.13. Confirmation](#2.2.6.13)\n",
    "- [3. Regression Benchmarking](#3)\n",
    "    - [3.1. Feature engineering](#3.1)\n",
    "        - [3.1.1 (Re)check correlation](#3.1.1)\n",
    "        - [3.1.2 Chi-Square for categorical data](#3.1.2)\n",
    "        - [3.1.3. Change columns](#3.1.3)\n",
    "    - [3.2. Modelling (create a predictive model)](#3.2)\n",
    "        - [3.2.1. Scaling](#3.2.1)\n",
    "        - [3.2.2. Encoding](#3.2.2)\n",
    "        - [3.2.3. Create a model](#3.2.3)\n",
    "            - [3.2.3.1. Linear Regression](#3.2.3.1)\n",
    "            - [3.2.3.2. Linear Regression Variants](#3.2.3.2)\n",
    "                - [3.2.3.2.1. Ridge Regression (L2 Regulization)](#3.2.3.2.1)\n",
    "                - [3.2.3.2.2. Lasso Regression (L1 Regulization)](#3.2.3.2.2)\n",
    "                - [3.2.3.2.3. Elastic Net Regression (L1 + L2 Regulization)](#3.2.3.2.3)\n",
    "            - [3.2.3.3. Decision Tree Regressor](#3.2.3.3)\n",
    "        - [3.2.4. Assess (evaluate model)](#3.2.4)\n",
    "            - [3.2.4.1. Atributes of Linear Regression](#3.2.4.1)\n",
    "            - [3.2.4.2. P-values](#3.2.4.2)\n",
    "            - [3.2.4.3. Comparing All Linear Regression Variant Models](#3.2.4.3)\n",
    "- [4. Open-Ended Section](#4)\n",
    "- [5. Deployment](#5)\n",
    "    - [5.1. Function with every change so far](#5.1)\n",
    "    - [5.2 Deploy (apply to real data)](#5.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07628065",
   "metadata": {},
   "source": [
    "<a id=\"1\">    </a>\n",
    "## 1. Identifying Business Needs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9bf1b6",
   "metadata": {},
   "source": [
    "- Overview and main goals of the project\n",
    "- Description of the overall process and identification of model assessment approach adopted in the work (CV, LOO, Holdout, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a94b29",
   "metadata": {},
   "source": [
    "<a id=\"2\">    </a>\n",
    "## 2. Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da85a93",
   "metadata": {},
   "source": [
    "- Description of data received -> key insights\n",
    "- Steps taken to clean and prepare the data based on exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f738f5",
   "metadata": {},
   "source": [
    "<a id=\"2.1\">    </a>\n",
    "### 2.1 Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2aa3db-2dcb-442f-98e7-dbb65366f544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6008c0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b9b0d7",
   "metadata": {},
   "source": [
    "<a id=\"2.1.1\">    </a>\n",
    "### 2.1.1. Inspecting data shape, data types and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12b1c40-68bb-4768-bbcd-2bc9d61659dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of rows and columns of the data\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce03213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types of each column\n",
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0877209e",
   "metadata": {},
   "source": [
    "- The variable year should be an object instead of float, since we are more interested in checking its statistics as a categorical variable.\n",
    "- The variable previous_owners should be integer instead of float.\n",
    "- The variable has_damage should be a boolean instead of float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ac2af-c6fa-4d7d-9fd4-ad610778bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing values\n",
    "train_data.replace('', np.nan, inplace=True)\n",
    "\n",
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad57eb",
   "metadata": {},
   "source": [
    "<a id=\"2.1.2\">    </a>\n",
    "### 2.1.2. Basic descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f019b-f8f8-4e7d-86e5-8e7dc245e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the basic descriptive statistics for the metric variables\n",
    "train_data.describe().round(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb668bf",
   "metadata": {},
   "source": [
    "By analyzing the describe table for the metric variables, we can identify that the distribution for most of them could potentially be skewed to the right:\n",
    "- **`price`** - is having a significative difference between **mean** and **median**, which indicates the distribution is not normal. The **max** value being far away from both mean and median could potentially represent outliers (perhaps luxury cars).\n",
    "- **`mileage`**, **`tax`**, **`mpg`**, **`engine_size`** and **`previous_owners`**- besides being also potentially skewed to the right and having outliers, given the distance between **mean**, **median** and **max**, it contains inconsistent values which are negative.\n",
    "- **`has_damage`**: is always 0 → no car has damage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the basic descriptive statistics for the categorical variables\n",
    "train_data.describe(include = 'O').round(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2963ce",
   "metadata": {},
   "source": [
    "Regarding our non metric features:\n",
    "- Ford and Focus are currently the most common **`brand`** and **`model`** in the dataset\n",
    "- Most of the cars are having a manual type of **`transmission`** and petrol as **`fuel type`**\n",
    "\n",
    "However we should confirm this after cleaning.\n",
    "\n",
    "Some inconsistencies:\n",
    " - **`transmission`**: should only have 3/4 unique values\n",
    " - **`fuel_type`**: should only have 4/5 unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f164d",
   "metadata": {},
   "source": [
    "<a id=\"2.1.3\">    </a>\n",
    "### 2.1.3. Identifying duplicated records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93a027-74f1-45c5-84a9-1f28fe154917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any duplicated observations\n",
    "train_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190cdf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display duplicated observations\n",
    "train_data[train_data.duplicated(keep=False)].sort_values(by=['brand', 'model', 'year', 'price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf06ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of data kept if we remove duplicated observations\n",
    "round((len(train_data.drop_duplicates())*100)/len(train_data), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64064eb3-83b4-4538-9a80-ab1c8350229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated observations\n",
    "def remove_duplicates(df):\n",
    "    ''' This function receives a DataFrame with the cars data as input and removes duplicated observations. '''\n",
    "    df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ab3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to dataset and confirm\n",
    "remove_duplicates(train_data)\n",
    "train_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b828903",
   "metadata": {},
   "source": [
    "- Even though the duplicated rows are associated to different car ids all the other columns are 100% identic. It is unusual to have two or more cars with the exact same mileage, tax and previous owners for example. This gives the perception that these cars were included in the system twice by mistake (human error perhaps). Given that, it makes sense to keep only the first occurrence of each duplicated row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff07ebf8",
   "metadata": {},
   "source": [
    "<a id=\"2.1.4\">    </a>\n",
    "### 2.1.4. Check the correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3578a7b2",
   "metadata": {},
   "source": [
    "Let's divide the columns into metric and non-metric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6fd104",
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_columns = train_data.columns.drop('price')\n",
    "categorical_columns = ['brand', 'model', 'transmission', 'fuel_type', 'year']\n",
    "numeric_columns = list(independent_columns.drop(categorical_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c4fc7",
   "metadata": {},
   "source": [
    "<a id=\"2.1.4.1\">    </a>\n",
    "#### 2.1.4.1. Correlation between the target and the numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a98467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the correlation matrix for the numerical columns\n",
    "correlation_list = numeric_columns + ['price']\n",
    "correlation_matrix = train_data[correlation_list].corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ed490",
   "metadata": {},
   "source": [
    "By observing the correlation between each independent numerical variables and the target variable (price), we can see that:\n",
    "- **mileage** - has a moderate inverse linear relationship, meaning that when the total reported distance traveled by the car (in miles) increases, the price has the tendency to decrease.\n",
    "- **tax** - is having a weak positive linear relationship, which indicates that when the amount of road tax increases, the price tends to increase slightly. This association is therefore not so pronounced.\n",
    "- **mpg** - on the other hand, presents a weak negative linear relationship: when the average miles per gallon increases, the price tends to decrease slightly.\n",
    "- **engine_size** - presents the strongest positive correlation with price: when the engine size increases the price tends to increase as well.\n",
    "- **previous_owners** and **has_damage** - these features have no linear relationship with price, meaning that these don't influence the price much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69312d8d",
   "metadata": {},
   "source": [
    "<a id=\"2.1.4.2\">    </a>\n",
    "#### 2.1.4.2. Correlation between the target and the categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b7d72",
   "metadata": {},
   "source": [
    "Let's test the null hypothesis that the mean is the same across all kinds of brand, model, transmission, fuel type, year and if it has damage, against the alternative that at least one differs from that mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7925d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_results = {}\n",
    "for cat_col in categorical_columns:\n",
    "    formula = f'price ~ C({cat_col})'                                                           # C indicates to the formula that the column is categorical\n",
    "    model = smf.ols(formula, data=train_data).fit()\n",
    "    anova_table = sm.stats.anova_lm(model)\n",
    "    anova_results[cat_col] = anova_table\n",
    "\n",
    "\n",
    "# Dataframe for the ANOVA results\n",
    "anova_results_df = pd.DataFrame()\n",
    "\n",
    "# Append each result obtained from the ANOVA test to the anova_results_df\n",
    "for cat_col, anova_table in anova_results.items():\n",
    "    temp_df = anova_table.copy()\n",
    "    temp_df['variable'] = cat_col                                                               # Save each category column name in a new column named variable\n",
    "    temp_df = temp_df.reset_index()                                                             # Convert index to column\n",
    "    anova_results_df = pd.concat([anova_results_df, temp_df], ignore_index=True)\n",
    "\n",
    "anova_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dc5977",
   "metadata": {},
   "source": [
    "The very high F values for each categorical independent feature suggest that all of these (brand, model, transmission, fuel_type, and year) variables explain a significant proportion of variance.\n",
    "For each feature, the p-value (0.0) is bellow any conventional significance level (ex.: 0.05), which indicates that there's a strong evidence against the null hypothesis that all brand, models, transmission, fuel_type and year prices means are equal. This means that they all have a big influence on the price, as the mean differs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a345754",
   "metadata": {},
   "source": [
    "<a id=\"2.2\">    </a>\n",
    "### 2.2. Modify the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acccbca-3a9b-4ec4-ad61-31e1c883d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values for each column of the data\n",
    "for name in categorical_columns:\n",
    "    print(f\"{name}: \\n{train_data[name].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928b0a6",
   "metadata": {},
   "source": [
    "Most of our variables are having inconsistent data:\n",
    "- brand for example is having multiple values for the same brand, such as BMW (BMW, BM, MW): we need to clean this variable.\n",
    "- model, similarly to brand is having too many unique values. We might need to group these values and classify them. \n",
    "- The same applies for transmission and fuel_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcfd99a",
   "metadata": {},
   "source": [
    "<a id=\"2.2.1\">    </a>\n",
    "### 2.2.1. Correct categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[categorical_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e71f8e",
   "metadata": {},
   "source": [
    "<a id=\"2.2.1.1\">    </a>\n",
    "#### 2.2.1.1. fuel_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836a049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.fuel_type.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a513dff",
   "metadata": {},
   "source": [
    "##### Possible Types of Fuel\n",
    "\n",
    "- Petrol\n",
    "- Diesel\n",
    "-  Hybrid\n",
    "- Electric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da11165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to standardize fuel types\n",
    "def replace_fuel(fuel_given):\n",
    "    \"\"\"Replaces various representations of fuel types with standardized names.\"\"\"\n",
    "\n",
    "    fuels = ['Petrol', 'Hybrid', 'Diesel', 'Unknown', 'Other', 'Electric']      # List of possible fuels                \n",
    "\n",
    "    if pd.isna(fuel_given):                                                     # In case of missing value keep as NaN\n",
    "        return np.nan\n",
    "    \n",
    "    # Else standardize fuel names     \n",
    "    fuel_given = fuel_given.upper().strip()        # Convert to uppercase for easier matching and remove leading/trailing spaces\n",
    "    \n",
    "    new_fuel = []                                                               # List of fuel types that matched with the fuel_given\n",
    "    \n",
    "    for fuel in fuels:                                                          # Check if the given fuel is one of the existing options\n",
    "        if fuel_given in fuel.upper():\n",
    "            new_fuel.append(fuel)\n",
    "    \n",
    "    if len(new_fuel)==1:                                                        # If there's only one possible option return it\n",
    "        return new_fuel[0]\n",
    "    else:                                     # Else we would get a new unique value telling us what kind of mistake to look for\n",
    "        return f'Check: {fuel_given}'                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b719a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function before applying it to the dataset\n",
    "fuel_types = train_data['fuel_type'].unique()\n",
    "for fuel in fuel_types:\n",
    "    print(f\"{fuel} => {replace_fuel(fuel)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to standardize fuel type in the 'fuel type' column\n",
    "train_data['fuel_type'] = train_data['fuel_type'].apply(replace_fuel)\n",
    "train_data['fuel_type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b744b5f",
   "metadata": {},
   "source": [
    "<a id=\"2.2.1.2\">    </a>\n",
    "#### 2.2.1.2. transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a5cf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.transmission.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869c052",
   "metadata": {},
   "source": [
    "##### Possible Types of Transmission in our dataset\n",
    "\n",
    "- Manual\n",
    "- Automatic\n",
    "- Semi-Automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b111cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to standardize transmission types\n",
    "def replace_transmission(transmission_given):\n",
    "    \"\"\"Replaces various representations of transmission types with standardized names.\"\"\"\n",
    "\n",
    "    transmissions = ['Semi-Auto', 'Automatic', 'Manual', 'Unknown', 'Other']            # Possible car transmissions\n",
    "\n",
    "    if pd.isna(transmission_given):                                                     # In case of missing value keep as NaN\n",
    "        return np.nan\n",
    "       \n",
    "    # Else standardize transmission names     \n",
    "    transmission_given = transmission_given.upper().strip()        # Convert to uppercase for easier matching and remove leading/trailing spaces\n",
    "    \n",
    "    new_transmission = []                                                               # List of transmissions that match with the transmission_given\n",
    "    \n",
    "    for transmission in transmissions:                                                  # Check if the given transmission is one of the existing options\n",
    "        if transmission_given in transmission.upper():\n",
    "            new_transmission.append(transmission)\n",
    "    \n",
    "    if len(new_transmission)==1:                                                        # If there's only one possible option return it\n",
    "        return new_transmission[0]\n",
    "    else:                                                           # Else we would get a new unique value telling us what kind of mistake to look for\n",
    "        return f'Check: {transmission_given}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function before applying it to the dataset\n",
    "transmission_types = train_data['transmission'].unique()\n",
    "for transmission in transmission_types:\n",
    "    print(f\"{transmission} => {replace_transmission(transmission)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd5e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to standardize transmission type in the 'transmission' column\n",
    "train_data['transmission'] = train_data['transmission'].apply(replace_transmission)\n",
    "train_data['transmission'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5423f1ea",
   "metadata": {},
   "source": [
    "<a id=\"2.2.1.3\">    </a>\n",
    "#### 2.2.1.3. brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6556c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.brand.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a772e1",
   "metadata": {},
   "source": [
    "##### Possible Brands in our dataset:\n",
    "- VW\n",
    "- BMW\n",
    "- Toyota\n",
    "- Audi\n",
    "- Ford\n",
    "- Skoda\n",
    "- Opel\n",
    "- Mercedes\n",
    "- Hyundai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of possible brands\n",
    "brands = ['VW', 'BMW', 'Toyota', 'Audi', 'Skoda', 'Opel', 'Ford', 'Mercedes', 'Hyundai', 'Unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756deb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to standardize brand types\n",
    "def replace_brand(brand_given):\n",
    "    \"\"\"Replaces various representations of brand types with standardized names.\"\"\"\n",
    "    \n",
    "    if pd.isna(brand_given):                                                              # In case of missing value keep as NaN\n",
    "        return np.nan\n",
    "       \n",
    "    # Else standardize brand names     \n",
    "    brand_given = brand_given.upper().strip()           # Convert to uppercase for easier matching and remove leading/trailing spaces\n",
    "    \n",
    "    new_brand = []\n",
    "    \n",
    "    for brand in brands:                                               # Check if the given brand is one of the existing options\n",
    "        if brand_given in brand.upper():\n",
    "            new_brand.append(brand)\n",
    "    \n",
    "    if len(new_brand)==1:                                                        # If there's only one possible option return it\n",
    "        return new_brand[0]\n",
    "    elif len(new_brand)>1 and 'VW' in new_brand:        # The else condition showed 'W' was raising an error, but with the code: train_data[(train_data.brand == 'w') | (train_data.brand == 'W')].model.unique(); we can see all W's are VW models\n",
    "        return 'VW'\n",
    "    else:                                     # Else we would get a new unique value telling us what kind of mistake to look for\n",
    "        return f'Check: {brand_given}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function before applying it to the dataset\n",
    "brand_types = train_data['brand'].unique()\n",
    "for brand in brand_types:\n",
    "    print(f\"{brand} => {replace_brand(brand)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to standardize brand names in the 'brand' column\n",
    "train_data['brand'] = train_data['brand'].apply(replace_brand)\n",
    "train_data['brand'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd14b48",
   "metadata": {},
   "source": [
    "<a id=\"2.2.1.4\">    </a>\n",
    "#### 2.2.1.4. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce386609",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.model.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87cfb2d",
   "metadata": {},
   "source": [
    "##### Possible Models in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54a3a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with every model for each brand\n",
    "brand_to_model = {\n",
    "    'VW': ['Golf', 'Golf S', 'Golf SV', 'Polo', 'Passat', 'Up', 'T-Roc', 'T-Crossland', 'Sharan', 'Tiguan', 'Touareg', 'Tiguan Allspace',\n",
    "           'Arteon', 'Amarok', 'Touran', 'Caddy', 'Caddy Life', 'Caddy Maxi', 'Caddy Maxi Life', 'Beetle', 'Shuttle',\n",
    "           'Caravelle', 'California', 'Scirocco', 'CC', 'Eos', 'Fox', 'Jetta'],\n",
    "    'BMW': ['1 Series', '2 Series', '3 Series', '4 Series', '5 Series', '6 Series', '7 Series', '8 Series', 'I',\n",
    "            'I3', 'I8', 'M', 'M3', 'M2', 'M4', 'M5', 'M6', 'X', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'Z', 'Z3',\n",
    "            'Z4'],\n",
    "    'Toyota': ['Yaris', 'Auris', 'C-HR', 'AYGO', 'RAV4', 'Prius', 'Corolla', 'Verso', 'Verso-S', 'Proace Verso', 'GT86',\n",
    "               'IQ', 'Land Cruiser', 'Urban Cruiser', 'Hilux', 'Avensis', 'Camry', 'Supra'],\n",
    "    'Audi': ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'Q2', 'Q3', 'Q5', 'Q7', 'Q8', 'S3', 'S5', 'TT', 'R8', 'RS',\n",
    "             'RS3', 'RS4', 'RS5', 'RS6', 'SQ5', 'SQ7', 'S4', 'S8'],\n",
    "    'Ford': ['Fiesta', 'Focus', 'Fusion', 'K', 'Ka', 'Ka+', 'EcoSport', 'Escort', 'B-Max', 'C-Max', 'Grand C-Max', 'S-Max',\n",
    "             'Mondeo', 'Mustang', 'Kuga', 'Tourneo Connect', 'Grand Tourneo Connect', 'Tourneo Custom', 'Galaxy', 'Puma',\n",
    "             'Edge', 'Streetka', 'Ranger'],\n",
    "    'Skoda': ['Octavia', 'Fabia', 'Rapid', 'Yeti', 'Yeti Outdoor', 'Scala', 'Kamiq', 'Kodiaq', 'Citigo', 'Roomster', 'Superb',\n",
    "              'Karoq'],\n",
    "    'Opel': ['Insignia', 'Mokka', 'Mokka X', 'Corsa', 'Cascada', 'Astra', 'Vectra', 'Viva', 'Vivaro', 'Ampera', 'Adam',\n",
    "             'Antara', 'Meriva', 'Crossland', 'Crossland X', 'Zafira', 'Zafira Tourer', 'Grandland', 'Grandland X',\n",
    "             'Combo Life', 'GTC', 'Kadjar', 'Tigra', 'Agila'],\n",
    "    'Mercedes': ['A-Class', 'B-Class', 'C-Class', 'E-Class', 'G-Class', 'M-Class', 'S-Class', 'V-Class', 'X-Class', 'CL-Class',\n",
    "                 'GL-Class', 'SL-Class', 'CLA-Class', 'CLC-Class', 'CLS-Class', 'CLK', 'GLA-Class', 'GLB-Class',\n",
    "                 'GLC-Class', 'GLS-Class', 'GLE-Class', 'SLK', '200', '220', '230'],\n",
    "    'Hyundai': ['i40', 'i30', 'i20', 'i10', 'ix20', 'ix35', 'Accent', 'Tucson', 'Terracan', 'Kona', 'Ioniq', 'Santa Fe',\n",
    "                'i800', 'Getz', 'Veloster'],\n",
    "    'Unknown': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc38a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for all possible models\n",
    "models = ['Golf', 'Golf S', 'Golf SV', 'Polo', 'Passat', 'Up', 'T-Roc', 'T-Crossland', 'Sharan', 'Tiguan', 'Touareg', 'Tiguan Allspace',\n",
    "    'Arteon', 'Amarok', 'Touran', 'Caddy', 'Caddy Life', 'Caddy Maxi', 'Caddy Maxi Life', 'Beetle', 'Shuttle', 'Caravelle',\n",
    "    'California', 'Scirocco', 'CC', 'Eos', 'Fox', 'Jetta', '1 Series', '2 Series', '3 Series', '4 Series', '5 Series',\n",
    "    '6 Series', '7 Series', '8 Series', 'I', 'I3', 'I8', 'M', 'M3', 'M2', 'M4', 'M5', 'M6', 'X', 'X1', 'X2',\n",
    "    'X3', 'X4', 'X5', 'X6', 'X7', 'Z', 'Z3', 'Z4', 'Yaris', 'Auris', 'C-HR', 'AYGO', 'RAV4', 'Prius', 'Corolla', 'Verso',\n",
    "    'Verso-S', 'Proace Verso', 'GT86', 'IQ', 'Land Cruiser', 'Urban Cruiser', 'Hilux', 'Avensis', 'Camry', 'Supra', 'A1', 'A2',\n",
    "    'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'Q2', 'Q3', 'Q5', 'Q7', 'Q8', 'S3', 'S5', 'TT', 'R8', 'RS', 'RS3', 'RS4', 'RS5',\n",
    "    'RS6', 'SQ5', 'SQ7', 'S4', 'S8', 'Fiesta', 'Focus', 'Fusion', 'K', 'Ka', 'Ka+', 'EcoSport', 'Escort', 'B-Max', 'C-Max',\n",
    "    'Grand C-Max', 'S-Max', 'Mondeo', 'Mustang', 'Kuga', 'Tourneo Connect', 'Grand Tourneo Connect', 'Tourneo Custom',\n",
    "    'Galaxy', 'Puma', 'Edge', 'Streetka', 'Ranger', 'Octavia', 'Fabia', 'Rapid', 'Yeti', 'Yeti Outdoor', 'Scala', 'Kamiq',\n",
    "    'Kodiaq', 'Citigo', 'Roomster', 'Superb', 'Karoq', 'Insignia', 'Mokka', 'Mokka X', 'Corsa', 'Cascada', 'Astra', 'Vectra',\n",
    "    'Viva', 'Vivaro', 'Ampera', 'Adam', 'Antara', 'Meriva', 'Crossland', 'Crossland X', 'Zafira', 'Zafira Tourer', 'Grandland',\n",
    "    'Grandland X', 'Combo Life', 'GTC', 'Kadjar', 'Tigra', 'Agila', 'A-Class', 'B-Class', 'C-Class', 'E-Class', 'G-Class', 'M-Class',\n",
    "    'S-Class', 'V-Class', 'X-Class', 'CL-Class', 'GL-Class', 'SL-Class', 'CLA-Class', 'CLC-Class', 'CLS-Class', 'CLK',\n",
    "    'GLA-Class', 'GLB-Class', 'GLC-Class', 'GLS-Class', 'GLE-Class', 'SLK', '200', '220', '230', 'i40', 'i30', 'i20', 'i10',\n",
    "    'ix20', 'ix35', 'Accent', 'Tucson', 'Terracan', 'Kona', 'Ioniq', 'Santa Fe', 'i800', 'Getz', 'Veloster', 'Unknown', 'NaN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98090a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate distance between two words\n",
    "def word_dist(bigger_word, smaller_word):                         # bigger_word = word without mistakes; smaller_word = word with potential mistakes\n",
    "    \"\"\"Returns a number representative of how different the words are.\"\"\"\n",
    "    \n",
    "    bigger_word = list(bigger_word.lower())\n",
    "    smaller_word = list(smaller_word.lower())\n",
    "    \n",
    "    for word in [bigger_word, smaller_word]:                      # Remove spaces\n",
    "        while ' ' in word:\n",
    "            word.remove(' ')\n",
    "    \n",
    "    if len(bigger_word) < len(smaller_word):                      # If the word that is supposed to be smaller (because it has mistakes) actually isn't, return len of smaller word (because there is definitly a word closer to it)\n",
    "        return len(smaller_word)\n",
    "    \n",
    "    for letter in smaller_word:                                   # If one of the letters in the \"wrong\" word isn't in the bigger word, it can't be the right model, return len of bigger word\n",
    "        if letter not in bigger_word:\n",
    "            return len(bigger_word)\n",
    "    \n",
    "    count = 0                                                     # Count how many letters are in common\n",
    "    for letter in bigger_word:\n",
    "        if letter in smaller_word:\n",
    "            count += 1\n",
    "            smaller_word.remove(letter)                           # No repetitions\n",
    "    \n",
    "    return len(bigger_word) - count + len(smaller_word)           # Adds len(smaller_word) because the remaining letters after the count are also different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c640c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_model(model_given, brand):\n",
    "    \"\"\"Replaces various representations of car models with standardized names.\"\"\"\n",
    "\n",
    "    if pd.isna(model_given):                                                              # In case of missing value keep as NaN\n",
    "        return np.nan\n",
    "    \n",
    "    # Else standardize model names\n",
    "    model_given = model_given.upper().strip()            # Convert to uppercase for easier matching and remove leading/trailing spaces\n",
    "    \n",
    "    if model_given.lower() == 'Unknown'.lower():                  # If model is 'Unknown'\n",
    "        return 'Unknown'\n",
    "    \n",
    "    new_models = models\n",
    "    brands = list(brand_to_model.keys())\n",
    "    brands.remove('Unknown')\n",
    "    \n",
    "    if brand in brands:                                           # If we know the brand, reduce options for model\n",
    "        new_models = brand_to_model[brand]\n",
    "    \n",
    "    if model_given.capitalize() == new_models:\n",
    "        return model_given.capitalize()\n",
    "    \n",
    "    distances = []\n",
    "    for model in new_models:                                      # Calculate the distance to each of the model options\n",
    "        if distances == []:\n",
    "            distances.append((model, word_dist(model, model_given)))\n",
    "        else:\n",
    "            if distances[0][1] > word_dist(model, model_given):                        # If the current saved distance is bigger than the new one, change for the smaller one\n",
    "                distances = [(model, word_dist(model, model_given))]\n",
    "    \n",
    "    if (brand == 'Audi') and (model_given in ['A', 'Q', 'R', 'RS', 'SQ', 'S']):        # Audi has a lot of potential options for these letters so the correct one isn't obvious\n",
    "        return 'Unknown'\n",
    "    \n",
    "    return distances[0][0]                                        # Return the model with less distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b463a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function before applying it to the dataset\n",
    "uniques = train_data[~train_data[['model', 'brand']].duplicated()]\n",
    "for index in uniques.index:\n",
    "    id_brand = uniques[uniques.index == index].brand.item()\n",
    "    id_model = uniques[uniques.index == index].model.item()\n",
    "    if id_model != replace_model(id_model, id_brand):\n",
    "        print(f\"{id_model} => {replace_model(id_model, id_brand)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e595ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to standardize model names in the 'model' column\n",
    "train_data['model'] = train_data.apply(lambda row: replace_model(row['model'], row['brand']), axis=1)\n",
    "train_data['model'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7213485",
   "metadata": {},
   "source": [
    "<a id=\"2.2.2\">    </a>\n",
    "### 2.2.2. Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a7c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_data_types(df):\n",
    "    ''' This function receives a cars DataFrame and replaces the datatypes of the variables with inconsistent datatypes and values. '''\n",
    "    \n",
    "    # Change NaN values in year to 0 (impossible value)\n",
    "    df.year.replace(np.nan, 0, inplace=True)\n",
    "    \n",
    "    # Change dtype of year to str\n",
    "    df.year = df.year.astype(int).astype(str)\n",
    "\n",
    "    # Change NaN values in previous_owners to 20 (impossible value)\n",
    "    df.previous_owners.replace(np.nan, 20, inplace=True)\n",
    "\n",
    "    # Change dtype of previous_owners to int\n",
    "    df.previous_owners = df.previous_owners.astype(int)\n",
    "\n",
    "    # Change dtype of has_damage to bool\n",
    "    df.has_damage = df.has_damage.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b3ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to dataset and confirm\n",
    "change_data_types(train_data)\n",
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47856079",
   "metadata": {},
   "source": [
    "<a id=\"2.2.3\">    </a>\n",
    "### 2.2.3. Fix mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c00934",
   "metadata": {},
   "source": [
    "<a id=\"2.2.3.1\">    </a>\n",
    "#### 2.2.3. previous_owners, mileage, mpg, engine_size, tax: have negative values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e066e513",
   "metadata": {},
   "source": [
    "We have two ways to deal with these problem:\n",
    "  - replace by the absolute value\n",
    "  - replace by NaN and deal as if it's a missing value (mean/median or KNN Imputer)\n",
    "  \n",
    "After looking at the data we chose to do:\n",
    "  - absolute:\n",
    "     - previous_owners\n",
    "     - mpg\n",
    "     - tax\n",
    "  - NaN:\n",
    "     - mileage\n",
    "     - engine_size\n",
    "\n",
    "**`Note:`** Through a brief search we learnt that the world's smallest engine size is 49 cc single-cylinder (0.049 cubic decimeter)\n",
    "\n",
    "Sources:\n",
    "- \"Small but Mighty: Check Out Some of the Smallest Car Engines in the World if You Want to Become a Mechanic\": https://www.autotrainingcentre.com/blog/small-mighty-check-smallest-car-engines-world-mechanic/\n",
    "- \"The world’s smallest car engine is slower than a cyclist\": https://www.drive.com.au/caradvice/the-worlds-smallest-car-engine-is-slower-than-a-cyclist/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7fd3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_negatives(df):\n",
    "    \"\"\" This function receives a car's DataFrame and replaces the negative values of \n",
    "    previous_owners, mpg and tax with the absolute value. \n",
    "    And for mileage and engine size replaces with nan value.\"\"\"\n",
    "\n",
    "    df[['previous_owners', 'mpg', 'tax']] = df[['previous_owners', 'mpg', 'tax']].apply(abs)\n",
    "    df.loc[df.mileage < 0, 'mileage'] = np.nan\n",
    "    df.loc[df.engine_size < 0.049, 'engine_size'] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc634a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to dataset and confirm\n",
    "replace_negatives(train_data)\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee8e28b",
   "metadata": {},
   "source": [
    "<a id=\"2.2.3.3\">    </a>\n",
    "#### 2.2.3.3. has_damage: is always 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c304260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the price when has_damage = 0\n",
    "train_data[train_data.has_damage == 0].price.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17127c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the price when has_damage = 1\n",
    "train_data[train_data.has_damage].price.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb75dd",
   "metadata": {},
   "source": [
    "We were thinking if we could create a threshold for which we consider that it's 1, i.e., if the price is lower than x, we assume that the car has damage. But it doesn't work, because the lower price for the 1's is bigger than the lower price for the 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e855ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the correlation matrix for the numerical columns\n",
    "correlation_list = numeric_columns + ['price']\n",
    "correlation_matrix = train_data[correlation_list].corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f7edd",
   "metadata": {},
   "source": [
    "The has_damage column is having an almost null correlation with price, meaning that having damage or not influences the price little to nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14962eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(train_data, x = 'price', hue = 'has_damage', bins = 10, edgecolor='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d977934",
   "metadata": {},
   "source": [
    "As we can see in the above histplot, the distribution for False or Nan (True) is similar, which again means that there is no difference in price in having or not damage. We're choosing to replace every Nan value in has_damage for 0, so the column is irrelevant. We'll drop it once we do feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e364db99",
   "metadata": {},
   "source": [
    "<a id=\"2.2.4\">    </a>\n",
    "### 2.2.4. Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856eae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns.remove('has_damage')\n",
    "categorical_columns.append('has_damage')\n",
    "print(f'All columns: {list(independent_columns)} \\nNumerical: {numeric_columns} \\nCategorical: {categorical_columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1743a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ALL Numeric Variables' Histograms in one figure\n",
    "sp_rows = 2\n",
    "sp_cols = 3\n",
    "\n",
    "# Prepare figure. Create individual axes where each histogram will be placed\n",
    "fig, axes = plt.subplots(sp_rows, sp_cols, figsize=(20, 11))\n",
    "\n",
    "# Plot data\n",
    "# Iterate across axes objects and associate each histogram:\n",
    "for ax, feat in zip(axes.flatten(), numeric_columns):\n",
    "    ax.hist(train_data[feat], edgecolor='black')\n",
    "    ax.set_title(feat, y=-0.13, fontsize=13)\n",
    "    \n",
    "# Layout\n",
    "title = \"Numeric Variables' Histograms\"\n",
    "plt.suptitle(title, y=.94, fontsize=35)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cfe7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ALL Numeric Variables' Boxplots in one figure\n",
    "\n",
    "sp_rows = 2\n",
    "sp_cols = 3\n",
    "\n",
    "# Prepare figure. Create individual axes where each histogram will be placed\n",
    "fig, axes = plt.subplots(sp_rows, sp_cols, figsize=(20, 11))\n",
    "\n",
    "# Plot data\n",
    "# Iterate across axes objects and associate each boxplot:\n",
    "for ax, feat in zip(axes.flatten(), numeric_columns):\n",
    "    sns.boxplot(x=train_data[feat], ax=ax)\n",
    "    \n",
    "# Layout\n",
    "title = \"Numeric Variables' Box Plots\"\n",
    "plt.suptitle(title, y=.94, fontsize=35)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4cb685",
   "metadata": {},
   "source": [
    "`Mileage, tax, mpg, engine size:` have a few outliers that we can remove\n",
    "\n",
    "`Previous owners:` previously, we replaced the Nan values with an extreme value for us to handle it in the missing values section. Due to this replacement, we are now seeing this outlier, which we will ignore in this section and handle later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063fb4d6",
   "metadata": {},
   "source": [
    "<a id=\"2.2.4.1\">    </a>\n",
    "#### 2.2.4.1. Winsorizing the outliers in numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739343fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsor_bounds(s,k=1.5):\n",
    "    \"\"\" This function calculates the quartiles and interquartile to perform winsorizing in the outliers.\"\"\"\n",
    "\n",
    "    q1,q3 = s.quantile([0.25,0.75])\n",
    "    iqr = q3 - q1\n",
    "    return q1 - k*iqr, q3 + k*iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a46523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorizing(df):\n",
    "    \"\"\" This function receives a cars DataFrame and winsorizes the outliers \n",
    "    in numeric columns by calling the winsor_bounds function.\"\"\"\n",
    "\n",
    "    cont_for_winsor = [c for c in [\"mileage\",\"mpg\",\"engine_size\",\"tax\"] if c in df.columns]\n",
    "\n",
    "    for c in cont_for_winsor:\n",
    "        lo, hi = winsor_bounds(df[c].astype(float))\n",
    "        df[c] = df[c].clip(lower=lo, upper=hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a97061",
   "metadata": {},
   "outputs": [],
   "source": [
    "winsorizing(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9afbf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure that all the outliers were removed\n",
    "# Plot ALL Numeric Variables' Boxplots in one figure\n",
    "\n",
    "sp_rows = 2\n",
    "sp_cols = 3\n",
    "\n",
    "# Prepare figure. Create individual axes where each histogram will be placed\n",
    "fig, axes = plt.subplots(sp_rows, sp_cols, figsize=(20, 11))\n",
    "\n",
    "# Plot data\n",
    "# Iterate across axes objects and associate each boxplot:\n",
    "for ax, feat in zip(axes.flatten(), numeric_columns):\n",
    "    sns.boxplot(x=train_data[feat], ax=ax)\n",
    "    \n",
    "# Layout\n",
    "title = \"Numeric Variables' Box Plots\"\n",
    "plt.suptitle(title, y=.94, fontsize=35)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b25362b",
   "metadata": {},
   "source": [
    "<a id=\"2.2.5\">    </a>\n",
    "### 2.2.5. Data separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.loc[:, list(independent_columns)]\n",
    "y = train_data.loc[:,'price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size = 0.7, random_state = RSEED, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b2199",
   "metadata": {},
   "source": [
    "<a id=\"2.2.6\">    </a>\n",
    "### 2.2.6. Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f48934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn strings into the NaN in the categorical columns\n",
    "for column in categorical_columns:\n",
    "    X_train[column].replace('nan', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4a3a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring back the NaN values in year, previous_owners and has_damage\n",
    "X_train.year.replace('0', np.nan, inplace=True)\n",
    "X_train.previous_owners.replace(20, np.nan, inplace=True)\n",
    "X_train.has_damage.replace(True, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9caae4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2bc20",
   "metadata": {},
   "source": [
    "We need to fix: numeric and categorical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee898671",
   "metadata": {},
   "source": [
    "<a id=\"2.2.6.1\">    </a>\n",
    "#### 2.2.6.1. Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc2075",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = X_train.year\n",
    "mode_year = X_train.year.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4bf159",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mode_year.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fcc5f1",
   "metadata": {},
   "source": [
    "Replace for the mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc764093",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.year = X_train.year.fillna(mode_year.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f835b364",
   "metadata": {},
   "source": [
    "<a id=\"2.2.6.8\">    </a>\n",
    "#### 2.2.6.8. Has damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4d72cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the Nan values for 0\n",
    "X_train.has_damage.replace(np.nan, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d15e88",
   "metadata": {},
   "source": [
    "### KKN Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=5).fit(X_train[numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4f3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_data = imputer.transform(X_train[numeric_columns])\n",
    "filled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f87cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_data_df = pd.DataFrame(filled_data, index=X_train.index, columns=numeric_columns)\n",
    "filled_data_df['previous_owners'] = filled_data_df['previous_owners'].astype(int)\n",
    "filled_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea269875",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([filled_data_df, X_train[categorical_columns]], axis=1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd9ef5",
   "metadata": {},
   "source": [
    "<a id=\"2.2.6.9\">    </a>\n",
    "#### 2.2.6.9. Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which replaces the nan brands for the rows without nan models\n",
    "def clean_brand(df):\n",
    "    \"\"\" This function receives a car DataFrame as input and cleans the missing values \n",
    "    of brand in case that row as a model assigned and is not the only one in the DataFrame with that same model.\"\"\"\n",
    "\n",
    "    # Sort the dataframe by model and brand to easily identify similar entries\n",
    "    df_sorted = df.sort_values(by=['model', 'brand'], ascending=[True, True])\n",
    "\n",
    "    # Create a mask where 'model' is not NaN\n",
    "    mask = df_sorted['model'].notna()\n",
    "\n",
    "    # Forward fill only for rows where 'model' is not NaN and whose previous row is having the same model \n",
    "    df_sorted.loc[mask, 'brand'] = df_sorted.loc[mask].groupby('model')['brand'].ffill()              # Locate the brands for which model is not NaN and fill it with the above value\n",
    "    \n",
    "    df['brand'] = df_sorted['brand']                                                                  # Replace brand values for the corresponding indexes\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a3e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the nan brands for the rows with actual models\n",
    "X_train = clean_brand(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a2b044",
   "metadata": {},
   "source": [
    "We will replace the Nan values for Unknown, so we don't lose data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc59f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.brand.replace(np.nan, 'Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d4113",
   "metadata": {},
   "source": [
    "<a id=\"2.2.6.10\">    </a>\n",
    "#### 2.2.6.10. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec27e4c",
   "metadata": {},
   "source": [
    "We will replace the Nan values for Unknown, so we don't lose data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.model.replace(np.nan, 'Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7984802",
   "metadata": {},
   "source": [
    "<a id=\"2.2.6.11\">    </a>\n",
    "#### 2.2.6.11.Transmission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dea209",
   "metadata": {},
   "source": [
    "We will replace the Nan values for Unknown, so we don't lose data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.transmission.replace(np.nan, 'Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32286a18",
   "metadata": {},
   "source": [
    "<a id=\"2.2.6.12\">    </a>\n",
    "### 2.2.6.12. Fuel type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0defbe66",
   "metadata": {},
   "source": [
    "We will replace the Nan values for Unknown, so we don't lose data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d352c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fuel_type.replace(np.nan, 'Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a2975",
   "metadata": {},
   "source": [
    "<a id=\"2.2.6.13\">    </a>\n",
    "#### 2.2.6.13. Confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0100735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c45289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_missing_values(df, mode_year): #, means, medians\n",
    "    \"\"\" This function receives a car DataFrame (validation and test set), the mode of the year and a list of means and medians of the train set\n",
    "     and cleans its missing values by replacing them with mean and median of the train set in case of numerical variables or with the mode or 'Unknown' in case of categorical\"\"\"\n",
    "\n",
    "    # Turn strings into the NaN in the categorical columns\n",
    "    for column in categorical_columns:\n",
    "        df[column].replace('nan', np.nan, inplace=True)\n",
    "\n",
    "    # Bring back the NaN values in year, previous_owners and has_damage\n",
    "    df.year.replace('0', np.nan, inplace=True)\n",
    "    df.previous_owners.replace(20, np.nan, inplace=True)\n",
    "    df.has_damage.replace(True, np.nan, inplace=True)\n",
    "\n",
    "    # Fill missing values with KNN Imputer\n",
    "    filled_set_data = imputer.transform(df[numeric_columns])\n",
    "    filled_set_data_df = pd.DataFrame(filled_set_data, index=df.index, columns=numeric_columns)\n",
    "    filled_set_data_df['previous_owners'] = filled_set_data_df['previous_owners'].astype(int)\n",
    "    df = pd.concat([filled_set_data_df, df[categorical_columns]], axis=1)\n",
    "\n",
    "    # Fill the missing values in year with the mode of the train set\n",
    "    df.year = df.year.fillna(mode_year.iloc[0])\n",
    "\n",
    "    # Fill the missing values in has_damage with 0\n",
    "    df.has_damage.replace(np.nan, 0, inplace=True)\n",
    "\n",
    "    # Fill the missing values in brands using ffill in case that row is having a model assigned\n",
    "    df = clean_brand(df) \n",
    "\n",
    "    # Fill the remaining missing values in brands with 'Unknown'\n",
    "    df.brand.replace(np.nan, 'Unknown', inplace=True)\n",
    "\n",
    "    # Fill the missing values in models with 'Unknown'\n",
    "    df.model.replace(np.nan, 'Unknown', inplace=True)\n",
    "\n",
    "    # Fill the missing values in transmission with 'Unknown'\n",
    "    df.transmission.replace(np.nan, 'Unknown', inplace=True)\n",
    "\n",
    "    # Fill the missing values in fuel_type with 'Unknown'\n",
    "    df.fuel_type.replace(np.nan, 'Unknown', inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d438d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation = clean_missing_values(X_validation, mode_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b768a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9750074d",
   "metadata": {},
   "source": [
    "The 'Unknown' models might affect the modeling later. Let's filter them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e9dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with Unknown car models from the train set\n",
    "X_train = X_train[X_train['model'] != 'Unknown']\n",
    "y_train = y_train.loc[X_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59de0d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with Unknown car models from the validation set\n",
    "X_validation = X_validation[X_validation['model'] != 'Unknown']\n",
    "y_validation = y_validation.loc[X_validation.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8273e6a4",
   "metadata": {},
   "source": [
    "<a id=\"3\">    </a>\n",
    "## 3. Regression Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1e5d9b",
   "metadata": {},
   "source": [
    "- Explanation of model assessment strategy and metrics used\n",
    "- Feature Selection Strategy and results\n",
    "- Optimization efforts: presentation, results and discussion\n",
    "- Comparison of performance between candidate models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee9c634",
   "metadata": {},
   "source": [
    "<a id=\"3.1\">    </a>\n",
    "### 3.1. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4e5ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = X_train[numeric_columns]\n",
    "X_train_cat = X_train[categorical_columns]\n",
    "\n",
    "X_val_num = X_validation[numeric_columns]\n",
    "X_val_cat = X_validation[categorical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a0a68",
   "metadata": {},
   "source": [
    "<a id=\"3.1.1\">    </a>\n",
    "#### 3.1.1. (Re)check correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b9f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns = ['brand', 'model', 'transmission', 'fuel_type']).corr(method = 'spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff21a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the correlation matrix for the numerical columns\n",
    "correlation_list = numeric_columns+['price']\n",
    "correlation_matrix = train_data[correlation_list].corr()\n",
    "correlation_matrix\n",
    "\n",
    "# Prepare figure\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot heatmap of the correlation matrix\n",
    "sns.heatmap(data=correlation_matrix, annot=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cc5336",
   "metadata": {},
   "source": [
    "<a id=\"3.1.3\">    </a>\n",
    "#### 3.1.3. Change columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121b6abc",
   "metadata": {},
   "source": [
    "We create a new variable, car_age, that calculates how old the car is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['car_age'] = 2025 - X_train['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e36fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd9a2a",
   "metadata": {},
   "source": [
    "So, we are going to drop the column year, since the car_age is more useful for future calculations and easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae9ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop('year', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3470d94d",
   "metadata": {},
   "source": [
    "We are also going to drop the column has_damage, because all the values are 0 and it has a Nan correlation with the other variables (is irrelevant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e117afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop('has_damage', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b0d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated the numerical columns list\n",
    "numeric_columns.append('car_age')\n",
    "\n",
    "# Update the df with the numerical columns only\n",
    "X_train_num = X_train[numeric_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f2ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f0570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update categorical columns list\n",
    "categorical_columns.remove('has_damage')\n",
    "categorical_columns.remove('year')\n",
    "\n",
    "# Update the df with the categorical columns\n",
    "X_train_cat = X_train[categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae67f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_columns(df):\n",
    "    \"\"\" This function receives a cars DataFrame and adds a new column car_age which calculates the car's age given its year\n",
    "    and drops unnecessary and redundant columns such as has_damage and year\"\"\"\n",
    "\n",
    "    df['car_age'] = 2025 - df['year'].astype(int)\n",
    "    df.drop('year', axis=1, inplace=True)\n",
    "    df.drop('has_damage', axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f49d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation = change_columns(X_validation)\n",
    "X_val_num = X_validation[numeric_columns]\n",
    "X_val_cat = X_validation[categorical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4f897",
   "metadata": {},
   "source": [
    "<a id=\"3.2\">    </a>\n",
    "## 3.2. Modelling (create a predictive model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8f5c9",
   "metadata": {},
   "source": [
    "<a id=\"3.2.1\">    </a>\n",
    "### 3.2.1. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9520bcfd",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call function\n",
    "scaler_MinMax = MinMaxScaler()\n",
    "\n",
    "#fit to training data\n",
    "scaler_MinMax.fit(X_train_num)\n",
    "\n",
    "#transform the data\n",
    "X_train_num_MinMax = scaler_MinMax.transform(X_train_num) # this will return an array\n",
    "\n",
    "#show results\n",
    "X_train_num_MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ae0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the array to a pandas dataframe\n",
    "X_train_num_MinMax = pd.DataFrame(X_train_num_MinMax, columns = X_train_num.columns).set_index(X_train.index)\n",
    "X_train_num_MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece9e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating for validation\n",
    "X_val_num_MinMax = scaler_MinMax.transform(X_val_num)\n",
    "X_val_num_MinMax = pd.DataFrame(X_val_num_MinMax, columns = X_val_num.columns).set_index(X_validation.index)\n",
    "X_val_num_MinMax.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452530f9",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158d9f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call function\n",
    "scaler_Std = StandardScaler()\n",
    "\n",
    "#fit to training data\n",
    "scaler_Std.fit(X_train_num)\n",
    "\n",
    "#transform the data\n",
    "X_train_num_Std = scaler_Std.transform(X_train_num) # this will return an array\n",
    "\n",
    "#show results\n",
    "X_train_num_Std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1454735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the array to a pandas dataframe\n",
    "X_train_num_Std = pd.DataFrame(X_train_num_Std, columns = X_train_num.columns).set_index(X_train.index)\n",
    "X_train_num_Std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacccba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating for validation\n",
    "X_val_num_Std = scaler_Std.transform(X_val_num)\n",
    "X_val_num_Std = pd.DataFrame(X_val_num_Std, columns = X_val_num.columns).set_index(X_validation.index)\n",
    "X_val_num_Std.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0224cac9",
   "metadata": {},
   "source": [
    "#### Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db7b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call function\n",
    "scaler_Robust = RobustScaler()\n",
    "\n",
    "#fit to training data\n",
    "scaler_Robust.fit(X_train_num)\n",
    "\n",
    "#transform the data\n",
    "X_train_num_Robust = scaler_Robust.transform(X_train_num) # this will return an array\n",
    "\n",
    "#show results\n",
    "X_train_num_Robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06623a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the array to a pandas dataframe\n",
    "X_train_num_Robust = pd.DataFrame(X_train_num_Robust, columns = X_train_num.columns).set_index(X_train.index)\n",
    "X_train_num_Robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8151249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating for validation\n",
    "X_val_num_Robust = scaler_Robust.transform(X_val_num)\n",
    "X_val_num_Robust = pd.DataFrame(X_val_num_Robust, columns = X_val_num.columns).set_index(X_validation.index)\n",
    "X_val_num_Robust.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88934498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_score(model_given, ax):\n",
    "    model_min_max1 = model_given.fit(X_train_num_MinMax, y_train)\n",
    "    score_minmax = model_min_max1.score(X_val_num_MinMax, y_validation)\n",
    "\n",
    "    model_std = model_given.fit(X_train_num_Std, y_train)\n",
    "    score_std = model_std.score(X_val_num_Std, y_validation)\n",
    "\n",
    "    model_Robust = model_given.fit(X_train_num_Robust, y_train)\n",
    "    score_robust = model_Robust.score(X_val_num_Robust, y_validation)\n",
    "\n",
    "    x = ['MinMax[0-1]','Standard','Robust']\n",
    "    y = [score_minmax, score_std, score_robust]\n",
    "\n",
    "    sns.barplot(x = x, y = y, ax = ax)\n",
    "\n",
    "possible_models = [LinearRegression(), DecisionTreeRegressor(), ElasticNet(), Lasso(), Ridge()]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "\n",
    "for ax, model in zip(axes.flatten(), possible_models):\n",
    "    standard_score(model, ax)\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel(model)\n",
    "\n",
    "plt.suptitle(\"Scaling options\", y=0.94, fontsize=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe52ce3",
   "metadata": {},
   "source": [
    "Standard scaling presents the best scores among all the models, especially for ElasticNet, meaning that with this type of scaling the models correctly predict between 60 and 70% of the samples in the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f153a1",
   "metadata": {},
   "source": [
    "<a id=\"3.2.2\">    </a>\n",
    "### 3.2.2. Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f468c03",
   "metadata": {},
   "source": [
    "First let's encode our categorical variables using One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59362d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ohc = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad345ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder to encode the categorical features.\n",
    "ohc = OneHotEncoder(sparse_output=False,                                # sparse_output=False outputs a numpy array, not a sparse matrix\n",
    "                    drop=\"first\", handle_unknown='ignore')              # drop the first category of each feature when performing one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041fda5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names and create a DataFrame with the one-hot encoded categorical features (pass feature names)\n",
    "ohc_feat = ohc.fit_transform(X_train_ohc[categorical_columns])              # Fit to data then transform it\n",
    "ohc_feat_names = ohc.get_feature_names_out()                                # Get output feature names from the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81de268",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohc_X_train = pd.DataFrame(ohc_feat, index=X_train_ohc.index, columns=ohc_feat_names)\n",
    "ohc_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6b416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the independent numerical variables\n",
    "ohc_X_train = pd.concat([ohc_X_train, X_train_num_Std], axis=1)\n",
    "ohc_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77dbdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(df, df_num_scaled):\n",
    "    \"\"\"This function receives a car's dataframe and its numerical features scaled version.\n",
    "    It encodes the categorical features and then concatenates to the scaled numerical features. \"\"\"\n",
    "\n",
    "    df_ohc = df.copy()\n",
    "    ohc_feat = ohc.transform(df_ohc[categorical_columns])               # Transform data\n",
    "    ohc_feat_names = ohc.get_feature_names_out()\n",
    "    ohc_df = pd.DataFrame(ohc_feat, index=df_ohc.index, columns=ohc_feat_names)\n",
    "    ohc_df = pd.concat([ohc_df, df_num_scaled], axis=1)\n",
    "    \n",
    "    return ohc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f9ec1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Encode the validation set\n",
    "ohc_X_val = encoding(X_validation, X_val_num_Std)\n",
    "ohc_X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d62e2c5",
   "metadata": {},
   "source": [
    "<a id=\"3.2.3\">    </a>\n",
    "### 3.2.3. Create a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf84c006",
   "metadata": {},
   "source": [
    "<a id=\"3.2.3.1\">    </a>\n",
    "#### 3.2.3.1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a060962",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c83bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the train dataset\n",
    "lin_model.fit(ohc_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6174ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict results for validation\n",
    "predictions = lin_model.predict(ohc_X_val)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b82fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predicted results with real values\n",
    "val_results = pd.DataFrame({'y_true': y_validation.values.flatten(), 'y_pred': predictions.flatten()}, \n",
    "                           index=y_validation.index                                       #ensures we can map the predictions to each observation correctly\n",
    "                           )\n",
    "\n",
    "\n",
    "val_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca64e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "y_pred_train = lin_model.predict(ohc_X_train)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "\n",
    "print('Training metrics:')\n",
    "print(f'R²: {r2_train:.4f}')\n",
    "print(f'MAE: {mae_train:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c1dd8d",
   "metadata": {},
   "source": [
    "- R²: About 83.9% of the variance in the price is explained by the features in the training dataset. It suggests that the model has captured a very small to almost null portion of the data's variability during training.\n",
    "\n",
    "- MAE: On average, the model's predictions of the price are off by approximately 2455.7£ from the actual values in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation metrics\n",
    "r2_val = r2_score(y_validation, predictions)\n",
    "mae_val = mean_absolute_error(y_validation, predictions)\n",
    "\n",
    "print('Validation metrics:')\n",
    "print(f'R²: {r2_val:.4f}')\n",
    "print(f'MAE: {mae_val:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc61677",
   "metadata": {},
   "source": [
    "- R²: 82.75% of the variance in the price is explained by the features in the validation data. This result doesn't fall much behind from the training set, meaning the model generalizes well to unseen data. \n",
    "- MAE: Predictions on the validation dataset have an average error of about 2459.6£ from the true values, which is also very close to the training. This indicates that the model performs well on both training and validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f836b4",
   "metadata": {},
   "source": [
    "<a id=\"3.2.3.2\">    </a>\n",
    "#### 3.2.3.2. Linear Regression Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c2171",
   "metadata": {},
   "source": [
    "<a id=\"3.2.3.2.1\">    </a>\n",
    "##### 3.2.3.2.1. Ridge Regression (L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: Create Ridge regression model with alpha=1.0\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "\n",
    "# Second: Fit the model\n",
    "ridge_model.fit(ohc_X_train, y_train)\n",
    "\n",
    "# Third: Make predictions\n",
    "y_pred_ridge_train = ridge_model.predict(ohc_X_train)\n",
    "y_pred_ridge_val = ridge_model.predict(ohc_X_val)\n",
    "\n",
    "# Fourth: Evaluate the model\n",
    "print(\"Ridge Regression Results:\")\n",
    "print(f\"Training R²: {r2_score(y_train, y_pred_ridge_train):.4f}\")\n",
    "print(f\"Validation R²: {r2_score(y_validation, y_pred_ridge_val):.4f}\")\n",
    "print(f\"Training MAE: {mean_absolute_error(y_train, y_pred_ridge_train):.2f}\")\n",
    "print(f\"Validation MAE: {mean_absolute_error(y_validation, y_pred_ridge_val):.2f}\")\n",
    "print(f\"\\nIntercept: {ridge_model.intercept_:.2f}\")\n",
    "print(\"Coefficients:\")\n",
    "for feature, coef in zip(X_train.columns, ridge_model.coef_):\n",
    "    print(f\"  {feature}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6836b34f",
   "metadata": {},
   "source": [
    "<a id=\"3.2.3.2.2\">    </a>\n",
    "##### 3.2.3.2.2. Lasso Regression (L1 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e338bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: Create Lasso regression model with alpha=1.0\n",
    "lasso_model = Lasso(alpha=1.0, max_iter=10000)\n",
    "\n",
    "# Second: Fit the model\n",
    "lasso_model.fit(ohc_X_train, y_train)\n",
    "\n",
    "# Third: Make predictions\n",
    "y_pred_lasso_train = lasso_model.predict(ohc_X_train)\n",
    "y_pred_lasso_val = lasso_model.predict(ohc_X_val)\n",
    "\n",
    "# Fourth: Evaluate the model\n",
    "print(\"Lasso Regression Results:\")\n",
    "print(f\"Training R²: {r2_score(y_train, y_pred_lasso_train):.4f}\")\n",
    "print(f\"Validation R²: {r2_score(y_validation, y_pred_lasso_val):.4f}\")\n",
    "print(f\"Training MAE: {mean_absolute_error(y_train, y_pred_lasso_train):.2f}\")\n",
    "print(f\"Validation MAE: {mean_absolute_error(y_validation, y_pred_lasso_val):.2f}\")\n",
    "print(f\"\\nIntercept: {lasso_model.intercept_:.2f}\")\n",
    "print(\"Coefficients:\")\n",
    "for feature, coef in zip(X_train.columns, lasso_model.coef_):\n",
    "    print(f\"  {feature}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e390db68",
   "metadata": {},
   "source": [
    "<a id=\"3.2.3.2.3\">    </a>\n",
    "##### 3.2.3.2.3. Elastic Net Regression (L1 + L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb5793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Elastic Net model with alpha=1.0 and l1_ratio=0.5 (equal mix of L1 and L2)\n",
    "elastic_model = ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=10000)\n",
    "\n",
    "# Fit the model\n",
    "elastic_model.fit(ohc_X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_elastic_train = elastic_model.predict(ohc_X_train)\n",
    "y_pred_elastic_val = elastic_model.predict(ohc_X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Elastic Net Regression Results:\")\n",
    "print(f\"Training R²: {r2_score(y_train, y_pred_elastic_train):.4f}\")\n",
    "print(f\"Validation R²: {r2_score(y_validation, y_pred_elastic_val):.4f}\")\n",
    "print(f\"Training MAE: {mean_absolute_error(y_train, y_pred_elastic_train):.2f}\")\n",
    "print(f\"Validation MAE: {mean_absolute_error(y_validation, y_pred_elastic_val):.2f}\")\n",
    "print(f\"\\nIntercept: {elastic_model.intercept_:.2f}\")\n",
    "print(\"Coefficients:\")\n",
    "for feature, coef in zip(X_train.columns, elastic_model.coef_):\n",
    "    print(f\"  {feature}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b5008",
   "metadata": {},
   "source": [
    "<a id=\"3.2.3.3\">    </a>\n",
    "#### 3.2.3.3. Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2280df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: Initialize the Decision Tree Regressor\n",
    "regressor = DecisionTreeRegressor(max_depth=4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8e83f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second: Train the model\n",
    "regressor.fit(ohc_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5360951",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohc_X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec0f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third: Predict on train set\n",
    "y_train_pred_tree = regressor.predict(ohc_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3be706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_tree = r2_score(y_train, y_train_pred_tree)\n",
    "print(f\"R-squared: {r2_tree}\")\n",
    "\n",
    "mae_tree = mean_absolute_error(y_train, y_train_pred_tree)\n",
    "print(f\"Mean Absolute Error: {mae_tree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c55662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth: Predict on the validation set\n",
    "y_val_pred_tree = regressor.predict(ohc_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf2027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_tree = r2_score(y_validation, y_val_pred_tree)\n",
    "print(f\"R-squared: {r2_tree}\")\n",
    "\n",
    "mae_tree = mean_absolute_error(y_validation, y_val_pred_tree)\n",
    "print(f\"Mean Absolute Error: {mae_tree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d8f3ca",
   "metadata": {},
   "source": [
    "<a id=\"3.2.4\">    </a>\n",
    "### 3.2.4 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7324c",
   "metadata": {},
   "source": [
    "<a id=\"3.2.4.1\">    </a>\n",
    "#### 3.2.4.1 Decision Tree Regressor - Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a37453",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_importance = DecisionTreeRegressor(random_state=RSEED).fit(ohc_X_train, y_train).feature_importances_\n",
    "mae_importance = DecisionTreeRegressor(criterion='absolute_error', random_state=RSEED).fit(ohc_X_train, y_train).feature_importances_\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Feature': ohc_X_train.columns,\n",
    "    'MSE': mse_importance,\n",
    "    'MAE': mae_importance\n",
    "})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ad9761",
   "metadata": {},
   "source": [
    "Let's group the results corresponded to each categorical feature by brand, model, transmission and fuel_type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad104e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original categorical column name to group by it and then sum the importance\n",
    "def group_by_cat_name(column):\n",
    "    \"\"\" This function receives a encoded categorical column name returns the categorical original column name to later group by it and calculate the sum of importance (mae and mse).\"\"\"\n",
    "    if column.startswith('model_'):\n",
    "        return 'model'\n",
    "    if column.startswith('brand_'):\n",
    "        return 'brand'\n",
    "    if column.startswith('transmission_'):\n",
    "        return 'transmission'\n",
    "    if column.startswith('fuel_type_'):\n",
    "        return 'fuel_type'\n",
    "    # If its a numerical column, return its own name\n",
    "    return column\n",
    "\n",
    "importance_df = results_df.assign(Feature = results_df['Feature'].map(group_by_cat_name)).groupby('Feature', as_index=False)[['MSE', 'MAE']].sum()\n",
    "\n",
    "importance_df.sort_values(by=['MAE', 'MSE'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1119651",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_melted = importance_df.melt(id_vars='Feature', var_name='Criterion', value_name='Importance')\n",
    "avg_order = importance_melted.groupby('Feature')['Importance'].mean().sort_values()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    data=importance_melted,\n",
    "    y='Feature',\n",
    "    x='Importance',\n",
    "    hue='Criterion',\n",
    "    order=avg_order.index\n",
    ")\n",
    "\n",
    "# add vertical threshold line at 0.05\n",
    "threshold = 0.05\n",
    "plt.axvline(x=threshold, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.title('Feature importance comparison (MSE vs MAE)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730a65c1",
   "metadata": {},
   "source": [
    "So from this feature importance analysis we can conclude the following:\n",
    "- To minimize MSE, the key drivers are orderly: **`transmission`**, **`engine_size`**, **`car_age`** and **`model`**. Together their summed importance tops 80%\n",
    "- To minimize MAE, the key drivers are orderly: **`car_age`**, **`transmission`**, **`engine_size`** and **`mileage`**.\n",
    "\n",
    "The remaining features, **`fuel_type`**, **`tax`**, **`previous_owners`** and **`brand`**, have all roughly less than 5% importance to minimize both MSE and MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e5bf8",
   "metadata": {},
   "source": [
    "<a id=\"3.2.4.2\">    </a>\n",
    "### 3.2.4.2. Attributes of Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22d3e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame(lin_model.coef_)\n",
    "coefs = coefs.set_index(ohc_X_train.columns)\n",
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b897437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check coefficients for all the car brands\n",
    "coefs[0: 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5406f947",
   "metadata": {},
   "source": [
    "According to the calculated coefficients:\n",
    "- When the brand is BMW, the price is expected to decrease by approximately 10279.7£, compared to a reference brand.\n",
    "- When it's Hyundai it's expected to decrease the price by a considerable amount of 11227£.\n",
    "- When it's Mercedes it's expected to increase the price by 43.33£, the only car brand here that seems to increase the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc4dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercept of linear regression\n",
    "lin_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038beb8a",
   "metadata": {},
   "source": [
    "If all independent variables (features tax, mpg, engine_size, etc.) were zero, the model predicts that the estimated price would be 34794.98£."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234da175",
   "metadata": {},
   "source": [
    "<a id=\"3.2.4.3\">    </a>\n",
    "### 3.2.4.3. P-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with sklearn coefficients\n",
    "sk_coef = np.concatenate(([lin_model.intercept_], np.asarray(lin_model.coef_).ravel()))\n",
    "print('\\nSklearn intercept and coefficients:')\n",
    "print(pd.Series(sk_coef, index=['Intercept'] + list(ohc_X_train.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e55779",
   "metadata": {},
   "source": [
    "- An increase of engine_size and previous_owners, is expected to increase the price by 14309.7£ and 72.86£, respectively.\n",
    "- For each increase in car_age, the price tends to decrease 29944.05£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89184b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statsmodels (full statistical summary)\n",
    "# Align the indices of X and y\n",
    "ohc_X_train_aligned, y_train_aligned = ohc_X_train.align(y_train, join='inner', axis=0)\n",
    "\n",
    "X_sm = sm.add_constant(ohc_X_train_aligned)\n",
    "ols = sm.OLS(y_train_aligned, X_sm).fit()\n",
    "print(ols.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f867af8",
   "metadata": {},
   "source": [
    "Each car brand is having a high p-value (0.225) which indicates **there's no sufficient** statistical evidence in the dataset that suggests a significant relationship between these brands and price.\n",
    "\n",
    "Many models such as 200, 230, B-Class, B-Max also present a high p-value.\n",
    "\n",
    "Models such as M2, M3 and M4, however, seem to present a p-value (0.00) lower than the conventional significance level (0.05), suggesting that there's an extremely strong evidence of a strong significant relationship between these models and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9647fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the p-values and filter by significance (e.g., > 0.05)\n",
    "significant_vars = ols.pvalues[ols.pvalues > 0.05]\n",
    "\n",
    "# Filter to only those whose names start with 'model'\n",
    "significant_model_vars = significant_vars[significant_vars.index.str.startswith('model')]\n",
    "\n",
    "# Get the coefficients for these filtered models with small p-value\n",
    "significant_model_coefs = ols.params[significant_model_vars.index]\n",
    "\n",
    "significant_model_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b6b2a",
   "metadata": {},
   "source": [
    "All the above models have a high p-value for the relationship between them and the price. Meaning, their effect on price is likely close to zero, suggesting they are not a useful predictor in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60491d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "WRITE CONCLUSION ON WHAT FEATURES TO DROP.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b067673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to drop\n",
    "brands_columns = [col for col in ohc_X_train.columns if col.startswith('brand_')]\n",
    "\n",
    "remove_features = ['previous_owners'] + brands_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop features we deemed unnecessary during feature selection\n",
    "def drop_unnecessary_features(df, features_to_drop):\n",
    "    \"\"\" This function receives a car DataFrame and a list of unnecessary features selected during Feature Selection and drops those features from the dataframes.\"\"\"\n",
    "    \n",
    "    df = df.drop(columns=features_to_drop)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b66534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary features from train and validation sets\n",
    "ohc_X_train = drop_unnecessary_features(ohc_X_train, remove_features)\n",
    "ohc_X_val = drop_unnecessary_features(ohc_X_val, remove_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e1450",
   "metadata": {},
   "source": [
    "<a id=\"3.2.4\">    </a>\n",
    "## 3.2.4. Assess (evaluate model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b050ad73",
   "metadata": {},
   "source": [
    "<a id=\"3.2.4.3\">    </a>\n",
    "### 3.2.4.1. Comparing All Linear Regression Variant Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f04fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Ridge', 'Lasso', 'Elastic Net', 'DecisionTreeRegressor'],\n",
    "    'Train R²': [\n",
    "        r2_score(y_train, y_pred_train),\n",
    "        r2_score(y_train, y_pred_ridge_train),\n",
    "        r2_score(y_train, y_pred_lasso_train),\n",
    "        r2_score(y_train, y_pred_elastic_train),\n",
    "        r2_score(y_train, y_train_pred_tree)\n",
    "    ],\n",
    "    'Val R²': [\n",
    "        r2_score(y_validation, lin_model.predict(ohc_X_val)),\n",
    "        r2_score(y_validation, y_pred_ridge_val),\n",
    "        r2_score(y_validation, y_pred_lasso_val),\n",
    "        r2_score(y_validation, y_pred_elastic_val),\n",
    "        r2_score(y_validation, y_val_pred_tree)\n",
    "    ],\n",
    "    'R² difference': [\n",
    "        abs(r2_score(y_train, y_pred_train) - r2_score(y_validation, lin_model.predict(ohc_X_val))),\n",
    "        abs(r2_score(y_train, y_pred_ridge_train) - r2_score(y_validation, y_pred_ridge_val)),\n",
    "        abs(r2_score(y_train, y_pred_lasso_train) - r2_score(y_validation, y_pred_lasso_val)),\n",
    "        abs(r2_score(y_train, y_pred_elastic_train) - r2_score(y_validation, y_pred_elastic_val)),\n",
    "        abs(r2_score(y_train, y_train_pred_tree) - r2_score(y_validation, y_val_pred_tree))\n",
    "    ],\n",
    "    'Train MAE': [\n",
    "        mean_absolute_error(y_train, y_pred_train),\n",
    "        mean_absolute_error(y_train, y_pred_ridge_train),\n",
    "        mean_absolute_error(y_train, y_pred_lasso_train),\n",
    "        mean_absolute_error(y_train, y_pred_elastic_train),\n",
    "        mean_absolute_error(y_train, y_train_pred_tree)\n",
    "    ],\n",
    "    'Val MAE': [\n",
    "        mean_absolute_error(y_validation, lin_model.predict(ohc_X_val)),\n",
    "        mean_absolute_error(y_validation, y_pred_ridge_val),\n",
    "        mean_absolute_error(y_validation, y_pred_lasso_val),\n",
    "        mean_absolute_error(y_validation, y_pred_elastic_val),\n",
    "        mean_absolute_error(y_validation, y_val_pred_tree)\n",
    "    ],\n",
    "    'MAE difference': [\n",
    "        abs(mean_absolute_error(y_train, y_pred_train) - mean_absolute_error(y_validation, lin_model.predict(ohc_X_val))),\n",
    "        abs(mean_absolute_error(y_train, y_pred_ridge_train) - mean_absolute_error(y_validation, y_pred_ridge_val)),\n",
    "        abs(mean_absolute_error(y_train, y_pred_lasso_train) - mean_absolute_error(y_validation, y_pred_lasso_val)),\n",
    "        abs(mean_absolute_error(y_train, y_pred_elastic_train) - mean_absolute_error(y_validation, y_pred_elastic_val)),\n",
    "        abs(mean_absolute_error(y_train, y_train_pred_tree) - mean_absolute_error(y_validation, y_val_pred_tree))\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57c906c",
   "metadata": {},
   "source": [
    "#### Overall Interpretation:\n",
    "\n",
    "1. **Linear Regression**:\n",
    "    - R²: Indicates that the model captures approximately 83.92% of the variance in training and about 82.75% in validation. This suggests our model fits well in both datasets.\n",
    "    - MAE: Both errors are also similar.\n",
    "2. **Ridge**:\n",
    "    - R²: Points to a similar performance as Linear but slightly reduced, suggesting Ridge might not significantly perform better.\n",
    "    - MAE:  Very similar to Linear but with a small increase in validation error.\n",
    "3. **Lasso**:\n",
    "    - R²: Presents a small decreased performance relative to Linear Regression and Ridge, which might indicate a loss of predictive power.\n",
    "    - MAE: Similarly, its errors increased slightly compared to the previous models.\n",
    "4. **Elastic Net**: \n",
    "    - R²: Represents the smallest, capture of variance (33.25%), for both the training and validation set, suggesting this model fits the data more poorly.\n",
    "    - MAE: Its MAE is also the highest among the various models, for both train and validation set.\n",
    "5. **Decision Tree Regressor**:\n",
    "    - R²: its results are much better than Elastic Net, capturing a variance of 70.56% in train and 70.29% in the validation, but falls behind the remaining models.\n",
    "    - MAE: Although it obtain less margin of error than Elastic Net it still doesn't outperformed the other models.\n",
    "\n",
    "Linear Regression seems to be at this phase the most suited model as it obtained similar results between train and validation, indicating the model generalizes well to unseen data. Its MAE is also the smallest among all the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83629ca",
   "metadata": {},
   "source": [
    "### 3.2.5 KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f411f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KNeighborsRegressor instance\n",
    "modelKNN_Reg = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4701b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the train se\n",
    "modelKNN_Reg.fit(X = ohc_X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042986e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on both train and validation set\n",
    "labels_train = modelKNN_Reg.predict(ohc_X_train)\n",
    "labels_val = modelKNN_Reg.predict(ohc_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f08b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the score for the train set\n",
    "modelKNN_Reg.score(ohc_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the score for the validation set\n",
    "modelKNN_Reg.score(ohc_X_val, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41807d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean absolute error for each set\n",
    "print(mean_absolute_error(y_train, labels_train))\n",
    "print(mean_absolute_error(y_validation, labels_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa3e767",
   "metadata": {},
   "source": [
    "Let's look for the best solution by identifying the optimal number of neighbors to select:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f8ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numberK_list=np.arange(1,15)\n",
    "# low_MAE=1000\n",
    "# nof=0   \n",
    "# score_list_train = []\n",
    "# score_list_val = []        \n",
    "# mae_list_train =[]\n",
    "# mae_list_val =[]\n",
    "# for n in numberK_list:\n",
    "\n",
    "#     model = KNeighborsRegressor(n_neighbors = n).fit(ohc_X_train, y_train)\n",
    "\n",
    "#     pred_train_knn = model.predict(ohc_X_train)\n",
    "#     pred_val_knn = model.predict(ohc_X_val)\n",
    "\n",
    "#     score_train = model.score(ohc_X_train, y_train)\n",
    "#     score_val = model.score(ohc_X_val, y_validation)\n",
    "\n",
    "#     score_list_train.append(score_train)\n",
    "#     score_list_val.append(score_val)\n",
    "\n",
    "#     error_train = mean_absolute_error(y_train, pred_train_knn)\n",
    "#     error_val = mean_absolute_error(y_validation, pred_val_knn)\n",
    "\n",
    "#     mae_list_train.append(error_train)\n",
    "#     mae_list_val.append(error_val)\n",
    "\n",
    "#     print('Number of neighbors: ' + str(n) + '\\n')\n",
    "#     print('Train score: ' + str(round(score_train, 4)))\n",
    "#     print('Validation score ' + str(round(score_val, 4)))\n",
    "#     print(\"Score difference train and validation:\", str(round(score_train - score_val, 4))+ '\\n')\n",
    "#     print('Train Mean absolute error: ' + str(round(error_train, 4)))\n",
    "#     print('Validation Mean absolute error: ' + str(round(error_val, 4)))\n",
    "#     print(\"MAE difference train and validation:\", str(round(error_val - error_train, 4)))\n",
    "#     print('....................')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af2b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(numberK_list, mae_list_train, label='Train')\n",
    "# plt.plot(numberK_list, mae_list_val, label = 'Validation')\n",
    "# plt.xticks(numberK_list)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92947d19",
   "metadata": {},
   "source": [
    "After K = 7, the MAE starts to increase for both train and validation set.\n",
    "\n",
    "We'll go with k = 6 as both sets seem to be closer to each other in this point before the start of the MAE increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4582aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNR_model = KNeighborsRegressor(n_neighbors = 6, p=1).fit(ohc_X_train, y_train)\n",
    "\n",
    "#pred_train_knr = KNR_model.predict(ohc_X_train)\n",
    "#pred_val_knr = KNR_model.predict(ohc_X_val)\n",
    "\n",
    "#knr_score_train = KNR_model.score(ohc_X_train, y_train)\n",
    "#knr_score_val = KNR_model.score(ohc_X_val, y_validation)\n",
    "\n",
    "#knr_error_train = mean_absolute_error(y_train, pred_train_knr)\n",
    "#knr_error_val = mean_absolute_error(y_validation, pred_val_knr)\n",
    "\n",
    "# Dataframe to display these values\n",
    "#pd.DataFrame({\n",
    "    #'Set' : ['Train', 'Validation'],\n",
    "    #'Score' : [knr_score_train, knr_score_val],\n",
    "    #'MAE' : [knr_error_train, knr_error_val]\n",
    "#})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e124bd5",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d945cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estim_list=np.arange(200, 401, 50)\n",
    "\n",
    "# score_list_train = []\n",
    "# score_list_val = []        \n",
    "# mae_list_train =[]\n",
    "# mae_list_val =[]\n",
    "# for n in n_estim_list:\n",
    "\n",
    "#     model = RandomForestRegressor(n_estimators = n, max_depth=None, random_state=RSEED, n_jobs=-1).fit(ohc_X_train, y_train)\n",
    "\n",
    "#     pred_train_rf = model.predict(ohc_X_train)\n",
    "#     pred_val_rf = model.predict(ohc_X_val)\n",
    "\n",
    "#     score_train = model.score(ohc_X_train, y_train)\n",
    "#     score_val = model.score(ohc_X_val, y_validation)\n",
    "\n",
    "#     score_list_train.append(score_train)\n",
    "#     score_list_val.append(score_val)\n",
    "\n",
    "#     error_train = mean_absolute_error(y_train, pred_train_rf)\n",
    "#     error_val = mean_absolute_error(y_validation, pred_val_rf)\n",
    "\n",
    "#     mae_list_train.append(error_train)\n",
    "#     mae_list_val.append(error_val)\n",
    "\n",
    "#     print('Number of estimators: ' + str(n) + '\\n')\n",
    "#     print('Train score: ' + str(round(score_train, 4)))\n",
    "#     print('Validation score ' + str(round(score_val, 4)))\n",
    "#     print(\"Score difference train and validation:\", str(round(score_train - score_val, 4))+ '\\n')\n",
    "#     print('Train Mean absolute error: ' + str(round(error_train, 4)))\n",
    "#     print('Validation Mean absolute error: ' + str(round(error_val, 4)))\n",
    "#     print(\"MAE difference train and validation:\", str(round(error_val - error_train, 4)))\n",
    "#     print('....................')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff86c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(n_estim_list, score_list_train, label='Train')\n",
    "# plt.plot(n_estim_list, score_list_val, label = 'Validation')\n",
    "# plt.xticks(n_estim_list)\n",
    "# plt.ylim(0.8, 1)                          # range 0.8 to 1.0\n",
    "# plt.yticks(np.arange(0.8, 1.01, 0.1))    # ticks every 0.05\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df960c69",
   "metadata": {},
   "source": [
    "- Using RandomizedSearchCV to find the best set of parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa0d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up the parameter distribution for RandomizedSearchCV\n",
    "\n",
    "# param_dist = {\n",
    "#     'n_estimators': randint(200, 400),\n",
    "#     'max_depth': [5, 7, 8, None],\n",
    "#     'min_samples_leaf': randint(1, 15),\n",
    "#     'min_samples_split': randint(2, 20),\n",
    "#     'max_features': ['sqrt', 0.5],\n",
    "#     'max_samples': uniform(0.6, 0.4),  # random number between 0.6 and 1.0\n",
    "#     'bootstrap': [True]\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe0d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestRegressor(random_state=RSEED)\n",
    "\n",
    "# search = RandomizedSearchCV(\n",
    "#     rf,\n",
    "#     param_dist,\n",
    "#     n_iter=40,\n",
    "#     cv=3,                \n",
    "#     scoring='neg_mean_absolute_error',\n",
    "#     n_jobs=-1,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# search.fit(ohc_X_train, y_train)\n",
    "# print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924668a5",
   "metadata": {},
   "source": [
    "- Used RandomForestRegressor best region as a starting point and then manually adjusted parameters (slightly more trees - we had establish that 300 was the best number of trees before RandomizedSearchCV through trials, stronger subsampling and slightly larger leaf size) to obtain the final model with the best validation performance (for now ). **Have to rewrite this later** + more explanation for parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_model = RandomForestRegressor(\n",
    "#     n_estimators=300,    \n",
    "#     max_depth=None,  # let trees grow fully \n",
    "#     max_features =  0.5,\n",
    "#     max_samples = 0.6,\n",
    "#     min_samples_leaf = 3,\n",
    "#     min_samples_split = 7,\n",
    "#     bootstrap = True,\n",
    "#     random_state=RSEED,   \n",
    "#     n_jobs=-1             \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00404d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_model.fit(X = ohc_X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984470ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict on both train and validation set\n",
    "# pred_train_rf = rf_model.predict(ohc_X_train)\n",
    "# pred_val_rf = rf_model.predict(ohc_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f12be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the score for the train set\n",
    "# rf_score_train = rf_model.score(ohc_X_train, y_train)\n",
    "\n",
    "# # Check the score for the validation set\n",
    "# rf_score_val   = rf_model.score(ohc_X_val, y_validation)\n",
    "\n",
    "# print(\"RandomForest - train R2:\", rf_score_train)\n",
    "# print(\"RandomForest - val   R2:\", rf_score_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec99533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean absolute error for each set\n",
    "# rf_error_train = mean_absolute_error(y_train, pred_train_rf)\n",
    "# rf_error_val   = mean_absolute_error(y_validation, pred_val_rf)\n",
    "\n",
    "# print(\"RandomForest - train MAE:\", rf_error_train)\n",
    "# print(\"RandomForest - val   MAE:\", rf_error_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48da26e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame({\n",
    "#     'Set':   ['Train', 'Validation'],\n",
    "#     'Score': [rf_score_train, rf_score_val],\n",
    "#     'MAE':   [rf_error_train,  rf_error_val]\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10362a4",
   "metadata": {},
   "source": [
    "### MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342f855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_param_dist = {\n",
    "#     'hidden_layer_sizes' : [(100,100,100), (50,100,50), (100,)],\n",
    "#     'activation': ['relu','tanh','logistic'],\n",
    "#     'max_iter': [300, 400, 500],\n",
    "#     'solver': ['adam','sgd'],\n",
    "#     'learning_rate_init': [0.001, 0.0001, 0.01],\n",
    "#     'batch_size': [32, 64, 128]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13168951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp = MLPRegressor(random_state=RSEED)\n",
    "\n",
    "# mlp_search = RandomizedSearchCV(\n",
    "#     mlp,\n",
    "#     mlp_param_dist,\n",
    "#     n_iter=40,\n",
    "#     cv=3,                \n",
    "#     scoring='neg_mean_absolute_error',\n",
    "#     n_jobs=-1,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# mlp_search.fit(ohc_X_train, y_train)\n",
    "# print(mlp_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc92d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPRegressor(\n",
    "    hidden_layer_sizes=(100,100,100), \n",
    "    activation='relu',      \n",
    "    solver='adam',             \n",
    "    max_iter=1000,    \n",
    "    learning_rate_init = 0.001,   \n",
    "    #batch_size= 50,      \n",
    "    random_state=42             \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0672d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.fit(X = ohc_X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ca276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on both train and validation set\n",
    "pred_train_mlp = mlp_model.predict(ohc_X_train)\n",
    "pred_val_mlp = mlp_model.predict(ohc_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41053efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 scores\n",
    "mlp_score_train = mlp_model.score(ohc_X_train, y_train)\n",
    "mlp_score_val = mlp_model.score(ohc_X_val, y_validation)\n",
    "\n",
    "print(\"MLP Regressor - train R2:\", mlp_score_train)\n",
    "print(\"MLP Regressor - val   R2:\", mlp_score_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd6c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE scores\n",
    "mlp_error_train = mean_absolute_error(y_train, pred_train_mlp)\n",
    "mlp_error_val = mean_absolute_error(y_validation, pred_val_mlp)\n",
    "\n",
    "print(\"MLP Regressor - train MAE:\", mlp_error_train)\n",
    "print(\"MLP Regressor - val   MAE:\", mlp_error_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0178a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'Set':   ['Train', 'Validation'],\n",
    "    'Score': [mlp_score_train, mlp_score_val],\n",
    "    'MAE':   [mlp_error_train,  mlp_error_val]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596fd84",
   "metadata": {},
   "source": [
    "<a id=\"4\">    </a>\n",
    "## 4. Open-Ended Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdc5bc",
   "metadata": {},
   "source": [
    "- Objectives for the Section and description of the actions taken\n",
    "- Results and discussion of main findings → key takeaways\n",
    "\n",
    "Note: This section expects that the objectives set go beyond what would reasonably be considered as adding or removing techniques to your pipeline. (e.g., using a feature selection technique not covered in class on your regular pipeline is not sufficient, but explicitly comparing different feature sets would be)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d8204",
   "metadata": {},
   "source": [
    "<a id=\"5\">    </a>\n",
    "## 5. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783d037",
   "metadata": {},
   "source": [
    "The final section of your work should implement the pipeline to generate reliable predictions for new data. The output should be the .csv file that you consider the solution you selected on Kaggle as your best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cfb509",
   "metadata": {},
   "source": [
    "<a id=\"5.1\">    </a>\n",
    "### 5.1. Function with every change so far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e94037",
   "metadata": {},
   "source": [
    "The test dataset will need to be clean before using it to evaluate the final model. Therefore we'll create a function to clean this dataset using the same clean treatment used in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_set(df, mode_year, numeric_columns_list, categorical_columns_list): #, means, medians\n",
    "    \"\"\" This functions receives a car's test set DataFrame and cleans it using the same cleaning treatment for the train set.\"\"\"\n",
    "    \n",
    "    # Drop the paintQuality% column from the test set\n",
    "    test_data.drop('paintQuality%', axis=1, inplace=True)\n",
    "    \n",
    "    # Reset numerical_columns and categorical columns\n",
    "    numeric_columns_list.remove('car_age')\n",
    "    categorical_columns_list.append('year')\n",
    "    categorical_columns_list.append('has_damage')\n",
    "\n",
    "\n",
    "    # First: rename the columns\n",
    "    rename_columns(df)\n",
    "\n",
    "    # Second: set the car_id and index\n",
    "    change_index(df)\n",
    "\n",
    "    # Third: change the datatypes of the variables year, previous_owners and has_damage\n",
    "    change_data_types(df)\n",
    "\n",
    "    \n",
    "    # Fourth: correct the values for the categorical columns:\n",
    "\n",
    "    # a) Replace fuel_type with standardize names\n",
    "    df['fuel_type'] = df['fuel_type'].apply(replace_fuel)\n",
    "\n",
    "    # b) Replace transmission with standardize names\n",
    "    df['transmission'] = df['transmission'].apply(replace_transmission)\n",
    "\n",
    "    # c) Replace brand with standardize names\n",
    "    df['brand'] = df['brand'].apply(replace_brand)\n",
    "\n",
    "    # d) Replace model with standardize names\n",
    "    df['model'] = df.apply(lambda row: replace_model(row['model'], row['brand']), axis=1)\n",
    "\n",
    "    # Fifth: fix inconsistent values in the numerical variables\n",
    "\n",
    "    # Replace the negative values in previous_owners, mpg, tax, mileage and engine_size \n",
    "    replace_negatives(df)\n",
    "\n",
    "    # Sixth: Remove outliers with winsorizing\n",
    "    winsorizing(df)\n",
    "\n",
    "    # Seventh: Fill missing values\n",
    "    df = clean_missing_values(df, mode_year) #, means, medians\n",
    "\n",
    "\n",
    "    # Eighth: Change columns\n",
    "    df = change_columns(df)\n",
    "    # Updated the numerical columns list\n",
    "    numeric_columns_list.append('car_age')\n",
    "    # Update categorical columns list\n",
    "    categorical_columns_list.remove('has_damage')\n",
    "    categorical_columns_list.remove('year')\n",
    "\n",
    "    # Ninth: Scale the numerical variables\n",
    "    X_test_num = df[numeric_columns_list]\n",
    "    X_test_num_scaled = scaler_Std.transform(X_test_num)\n",
    "    X_test_num_scaled_df = pd.DataFrame(X_test_num_scaled, columns=X_test_num.columns, index=X_test_num.index)\n",
    "\n",
    "    # Tenth: Encode the categorical variables\n",
    "    ohc_X_test = encoding(df, X_test_num_scaled_df)\n",
    "\n",
    "\n",
    "    return ohc_X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1304ea3",
   "metadata": {},
   "source": [
    "<a id=\"5.1\">    </a>\n",
    "### 5.2 Deploy (apply to real data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the test set using the same cleaning treatment for the training set\n",
    "test_cleaned = clean_set(test_data, mode_year, numeric_columns, categorical_columns) #, means, medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b4abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a934bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = mlp_model.predict(test_cleaned)\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame({'price': test_predictions.flatten()}, \n",
    "                           index=test_data.index #ensures we can map the predictions to each observation correctly\n",
    "                           )\n",
    "test_results.index.names = ['carID']\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103acd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.to_csv('Group14_test_pred.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
